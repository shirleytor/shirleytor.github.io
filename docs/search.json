[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shirley M. Toribio",
    "section": "",
    "text": "Hello there! Welcome to my website!\nI’m Shirley Toribio, an aspiring statistician and data scientist. I completed my Bachelor of Arts in Mathematics, with a concentration in Statistics, in December 2024. This website displays some of my past work.\n\n\n\nArt by me\n\n\nThe github repository hosting this website can be found here."
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "World Fairs and CO2 Emissions",
    "section": "",
    "text": "This analysis looks at world fairs. According to Wikipedia,\n\na world’s fair, also known as a universal exhibition or an expo, is a large global exhibition designed to showcase the achievements of nations. These exhibitions vary in character and are held in different parts of the world at a specific site for a period of time, typically between three and six months.\n\nDue to various factors, however, it is possible that world fairs might not be as representative of nations worldwide as their description and name would suggest. Ideally, as many nations as possible should be included in a world fair in order for it to be truly representative. As such, I track the number of countries that have attended world fairs from 1851 to 2021 to see how world fairs have evolved in terms of country representation.\n\n\n\n\n\n\n\n\n\nBased on the upward trend of the line of best fit, world fairs seem to have become more inclusive of countries as time has gone by. This trend towards increased inclusivity of countries seems to be especially present from 1975 to 2021. Modern world fair thus seem more representative of countries worldwide than world fairs of the past.\nData source here."
  },
  {
    "objectID": "data_viz.html#analyzing-data-on-world-fairs-from-1855-to-2022",
    "href": "data_viz.html#analyzing-data-on-world-fairs-from-1855-to-2022",
    "title": "World Fairs and CO2 Emissions",
    "section": "",
    "text": "This analysis looks at world fairs. According to Wikipedia,\n\na world’s fair, also known as a universal exhibition or an expo, is a large global exhibition designed to showcase the achievements of nations. These exhibitions vary in character and are held in different parts of the world at a specific site for a period of time, typically between three and six months.\n\nDue to various factors, however, it is possible that world fairs might not be as representative of nations worldwide as their description and name would suggest. Ideally, as many nations as possible should be included in a world fair in order for it to be truly representative. As such, I track the number of countries that have attended world fairs from 1851 to 2021 to see how world fairs have evolved in terms of country representation.\n\n\n\n\n\n\n\n\n\nBased on the upward trend of the line of best fit, world fairs seem to have become more inclusive of countries as time has gone by. This trend towards increased inclusivity of countries seems to be especially present from 1975 to 2021. Modern world fair thus seem more representative of countries worldwide than world fairs of the past.\nData source here."
  },
  {
    "objectID": "data_viz.html#total-carbon-dioxide-emissions-from-major-natural-resource-companies",
    "href": "data_viz.html#total-carbon-dioxide-emissions-from-major-natural-resource-companies",
    "title": "World Fairs and CO2 Emissions",
    "section": "Total carbon dioxide emissions from major natural resource companies",
    "text": "Total carbon dioxide emissions from major natural resource companies\nThis analysis looks at emissions of carbon dioxide from major natural resource companies. The dataset used for this analysis originates from a Carbon Majors database.\n\n\n\n\n\n\n\n\n\nBased on this graph, the production of oil, natural gas, and coal seems to be mostly responsible for the amount of carbon dioxide emitted by major natural resource companies. Major natural resource companies should thus look into reducing their production of these commodities and shifting towards production of other commodoties in order to reduce their carbon dioxide emissions.\nData source here."
  },
  {
    "objectID": "project3_ds.html",
    "href": "project3_ds.html",
    "title": "Simulation Study",
    "section": "",
    "text": "Symmetric beta distributions and normal distributions are very similar to each other in shape. Due to this, in this project I examine how a 95% prediction interval that assumes data originates from a normal distribution would fare in terms of prediction accuracy and actual coverage probability of beta-distributed data. I define a confidence interval with “good” prediction accuracy as one that covers the actual mean of the data’s parent distribution, and a confidence interval with good coverage probability as one that covers approximately .95 of the beta distribution. All data is generated from beta distributions with mean .5."
  },
  {
    "objectID": "project3_ds.html#introduction",
    "href": "project3_ds.html#introduction",
    "title": "Simulation Study",
    "section": "",
    "text": "Symmetric beta distributions and normal distributions are very similar to each other in shape. Due to this, in this project I examine how a 95% prediction interval that assumes data originates from a normal distribution would fare in terms of prediction accuracy and actual coverage probability of beta-distributed data. I define a confidence interval with “good” prediction accuracy as one that covers the actual mean of the data’s parent distribution, and a confidence interval with good coverage probability as one that covers approximately .95 of the beta distribution. All data is generated from beta distributions with mean .5."
  },
  {
    "objectID": "project3_ds.html#simulations",
    "href": "project3_ds.html#simulations",
    "title": "Simulation Study",
    "section": "Simulations",
    "text": "Simulations\nFirst, I tidy up.\n\nlibrary(tidyverse)\n\nThen, I define a few functions that will help me in my task.\n\nCI &lt;- function(data, coverage_prob){ \n  #Generates a normal prediction interval with an intended coverage probability of coverage_prob based on a vector of numeric data\n  \n  lower_zscore &lt;- qnorm((1-coverage_prob)/2)\n  upper_zscore &lt;- qnorm(((1-coverage_prob)/2) + coverage_prob)\n  avg &lt;- mean(data)\n  stan_d &lt;- sd(data)\n  lower_bound &lt;- avg + lower_zscore*stan_d\n  upper_bound &lt;- avg + upper_zscore*stan_d\n  return(data.frame(PI_percentage = coverage_prob, lower = lower_bound, upper = upper_bound))\n}\n\none_beta_simulation &lt;- function(n, alpha, beta, ci_prop){\n  #Assesses prediction accuracy and actual coverage probability of a normal prediction interval when used on a vector of numeric data of size n. The numeric data is generated from a beta distribution with parameters alpha and beta.\n  \n  cover_df &lt;- CI(rbeta(n, alpha, beta), ci_prop)\n  cover_prop &lt;- pbeta(cover_df[1, \"upper\"], alpha, beta) - pbeta(cover_df[1, \"lower\"], alpha, beta)\n  mean_in_interval &lt;- .5 &gt;= cover_df[1, \"lower\"] & .5 &lt;= cover_df[1,\"upper\"]\n  param_df &lt;- data.frame(cover = cover_prop, alpha = rep(alpha, nrow(cover_df)), beta = rep(beta, nrow(cover_df)), mean_in_interval = mean_in_interval)\n  df &lt;- cbind(cover_df, param_df)\n  return(df)\n}\n\nbeta_sims_n &lt;- function(n){\n  #Iterates over a vector of possible alpha = beta values and applies one_beta_simulation to each possible value of alpha/beta. All simulations use data of sample size n.\n  df1 &lt;- map(parameters,\\(param) one_beta_simulation(n, param, param, ci) ) %&gt;%\n  list_rbind()\n  df2 &lt;- data.frame(n = rep(n, nrow(df1)))\n  df &lt;- cbind(df2, df1)\n  return(df)\n}\n\nTime to simulate over different parameter values and sample sizes!\n\nparameters &lt;- seq(5, 200, by = 2)\nn &lt;- 2:500\nci &lt;- .95\n\nbeta_df &lt;- map(n, \\(n) beta_sims_n(n)) %&gt;%\n  list_rbind()\n\nThis is a glimpse at the results of the simulations.\n\nrows &lt;- sample(1:nrow(beta_df), 10)\nmap(rows, \\(i) beta_df[i,]) %&gt;%\n  list_rbind()\n\n     n PI_percentage     lower     upper     cover alpha beta mean_in_interval\n1  247          0.95 0.4523062 0.5547580 0.9355907   165  165             TRUE\n2   83          0.95 0.4359215 0.5669415 0.9650810   129  129             TRUE\n3  170          0.95 0.4418966 0.5511343 0.9487325   161  161             TRUE\n4  350          0.95 0.4462158 0.5532432 0.9619462   187  187             TRUE\n5  135          0.95 0.4451088 0.5587557 0.9621922   167  167             TRUE\n6  415          0.95 0.4475184 0.5524290 0.9621266   195  195             TRUE\n7  331          0.95 0.4430825 0.5614637 0.9531166   141  141             TRUE\n8  471          0.95 0.4038219 0.6013108 0.9345494    43   43             TRUE\n9  199          0.95 0.4199542 0.5844024 0.9165128    55   55             TRUE\n10 348          0.95 0.4325954 0.5677737 0.9326805    91   91             TRUE\n\n\nThis is a random sample of rows from the actual dataset, which has 48902 rows and 8 columns. Each row corresponds to a simulated random sample of size n from a beta distribution with parameters alpha and beta. For each random sample, a normal prediction interval was generated with bounds “lower” and “upper”. “PI_percentage” refers to the intended coverage probability and “cover” refers to the actual coverage probability of said prediction interval over the beta distribution the data was generated from. “mean_in_interval” is a binary variable that states if the prediction interval covered the mean of the distribution."
  },
  {
    "objectID": "project3_ds.html#insights",
    "href": "project3_ds.html#insights",
    "title": "Simulation Study",
    "section": "Insights",
    "text": "Insights\n\nn_means_df &lt;- beta_df %&gt;%\n  mutate(diff = cover - PI_percentage) %&gt;%\n  group_by(n) %&gt;%\n  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n()) %&gt;%\n  filter(n %in% 1:100)\n\nggplot(n_means_df, aes(x = n, y = mean)) + \n  geom_point() + \n  geom_hline(yintercept = 0, col = \"black\")  +\n  labs(\n    x = \"sample size\",\n    y = \"difference between actual and intended coverage\",\n    title = \"Figure 1\",\n    subtitle = \"difference between actual and intended coverage probability based on sample size\",\n  )\n\n\n\n\n\n\n\n\nFigure 1 graphs the mean difference between the actual coverage probability and intended coverage probability (calculated as: actual coverage probability - intended coverage probability) per each sample size from 2 to 100. A negative mean difference indicates the actual coverage probability tended to be less than the intended coverage probability, which is undesirable. For sample sizes from 2 to around 13, the mean differences tend to be much lower than 0, meaning the prediction interval is likely to cover less than intended. As the sample size increases, this mean difference seems to converge in probability to 0, meaning the actual coverage probability is more likely to match the intended coverage probability.\nFor small sample sizes it seems likely a normal prediction interval will cover less than intended, with it seeming to cover less the smaller the sample size is.\n\nn_means_df &lt;- beta_df %&gt;%\n  mutate(diff = cover - PI_percentage) %&gt;%\n  group_by(n) %&gt;%\n  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n()) %&gt;%\n  filter(n %in% 1:30)\n\nggplot(n_means_df, aes(x = n, y = mu_in_interval)) + \n  geom_point()  +\n  labs(\n    x = \"sample size\",\n    y = \"proportion of mu-inclusive prediction intervals\",\n    title = \"Figure 2\",\n    subtitle = \"proportion of mu-inclusive prediction intervals based on sample size\"\n  )\n\n\n\n\n\n\n\n\nFigure 2 plots the proportion of prediction intervals that cover mu = .5 per each sample size from 2 to 30. These points converge to 1 at a sample size of around 10, meaning for sample sizes of 10 or greater it is probable that all random samples with those samples sizes will produce normal-prediction intervals that cover the mean of the beta distribution said data originated from. Based on this plot, the normal-prediction interval fares well with accepting the null hypothesis of mu = .5 given the null distribution is symmetric and beta.\n\nns_of_interest &lt;- c(5, 10, 30, 50, 100, 500)\nbeta_df_2 &lt;- filter(beta_df,n %in% ns_of_interest) %&gt;%\n  mutate(sample_size = as.factor(n))\nggplot(beta_df_2, aes(x = alpha, y = cover, color = sample_size)) +\n  geom_point() +\n  geom_smooth(aes(line = n ), se = FALSE) +\n  labs(\n    x = \"value of alpha and beta\",\n    y = \"actual coverage probability\",\n    title = \"Figure 3\",\n    subtitle = \"actual coverage probability based on parameter values\"\n  )\n\n\n\n\n\n\n\n\nFigure 3 plots the actual coverage probability per each value of alpha and beta. Points and lines are colored based on sample size, which are described in the legend. Alpha and beta have the same values, by the way. All the lines of best fit seem close to being horizontal, meaning that there most likely is not a relationship between the values of alpha/beta and the actual coverage probability of the data. Based on the y-intercepts of each line and the vertical spread of points given sample size, normal prediction intervals based on small amounts of data seem more likely to deviate from the intended coverage probability of the data’s parent symmetric beta distribution and to deviate to being lower than .95."
  },
  {
    "objectID": "project3_ds.html#conclusion",
    "href": "project3_ds.html#conclusion",
    "title": "Simulation Study",
    "section": "Conclusion",
    "text": "Conclusion\nBy generating data from symmetric beta distributions with mu = .5 and making normal prediction intervals based on this data, I was able to assess the predictive accuracy and actual coverage probability of normal prediction intervals when applied to symmetric-beta data. Even for sample sizes as small as 10, normal prediction intervals seem to have good predictive accuracy, although their coverage probability is quite poor for sample sizes below 30. When the null hypothesis is mu = .5, it appears they are quite adept at avoiding type I errors."
  },
  {
    "objectID": "proj4.html",
    "href": "proj4.html",
    "title": "SQL",
    "section": "",
    "text": "I plan to query data from the Wideband Acoustic Immittance Database, which is a repository of auditory measurements from different people. From this data, two graphs will be generated. One graph will replicate Figure 1 from Voss(2020) and the other will compare mean absorbance measurements between people of different sexes over different frequencies.\n\nlibrary(RMariaDB)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n #collect(Measurements)\n\nYou can add options to executable code like this\n\nSELECT Measurements.Identifier,\n       Measurements.Frequency,\n       AVG(Absorbance) AS mean_absorbance,\n       PI_Info.AuthorsShortList AS authors\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nGROUP BY Measurements.Identifier, Measurements.Frequency;\n\n\ntable1 %&gt;%\n  filter(authors %in% c(\"Abur et al.\", \"Feeney et al.\", \"Groon et al.\", \"Lewis and Neely\", \"Liu et al.\", \"Rosowski et al.\", \"Shahnaz et al.\", \"Shaver and Sun\", \"Sun et al.\", \"Voss and Allen\", \"Voss et al.\", \"Werner et al.\")) %&gt;%\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n  geom_smooth(aes(color = authors), se = FALSE) +\n  xlim(0, 8000)\n\n\n\n\n\n\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\nSELECT\n  Subjects.Sex AS sex,\n  Subjects.Race AS race,\n  Subjects.Ethnicity AS ethnicity,\n  Subjects.Identifier,\n  Measurements.Identifier,\n  Measurements.Frequency AS freq,\n  AVG(Measurements.Absorbance) AS mean_absorbance\nFROM Subjects\nJOIN Measurements ON Subjects.SubjectNumber = Measurements.SubjectNumber \nWHERE Subjects.Identifier = \"Aithal_2013\" AND Measurements.Identifier = \"Aithal_2013\"\nGROUP BY ethnicity, race, sex, freq;\n\n\ntable2 &lt;- table2[,-c(4)]\ntable2 %&gt;%\n  ggplot(aes(x = freq, y = mean_absorbance, color = sex)) +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nUsing SQL queries, I filtered through and sorted data in a manner that allowed me to compare absorbance measurements across sexes and copy figure 1 of Voss(2020). I did this with 2 SQL queries and joined different tables from the same database to produce both graphs. Following each query, I used ggplot to plot mean absorbance measurements alongside frequency."
  },
  {
    "objectID": "SQL_Proj.html",
    "href": "SQL_Proj.html",
    "title": "Wideband Acoustic Immittance Data Visualization",
    "section": "",
    "text": "I plan to query data from the Wideband Acoustic Immittance Database, which is a repository of auditory measurements from different people. From this data, two graphs will be generated. One graph will replicate Figure 1 from Voss(2019) and the other will compare mean absorbance measurements between babies of different sexes over different frequencies using data from Aithal et al.(2013).\nThis is the abstract of Aithal et al.(2013):\n\nPresently, normative wideband reflectance data are available for neonates who have passed a distortion product otoacoustic emission test. However, passing the distortion product otoacoustic emission test alone does not ensure normal middle ear function. The objective of this study was to establish normative wideband reflectance data in healthy neonates with normal middle ear function, as justified by passing a battery of tests.\n\nVoss(2019) is a summarization of different studies analyzing auditory measurements among people.\n\nlibrary(RMariaDB)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n #collect(Measurements)\n\n\nSELECT Measurements.Identifier,\n       Measurements.Frequency,\n       AVG(Absorbance) AS mean_absorbance,\n       CONCAT(PI_Info.AuthorsShortList,\" (\", PI_Info.Year, \")\",\" N=\", COUNT(DISTINCT Measurements.SubjectNumber) ,\"; \",Measurements.Instrument) AS studies\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE PI_Info.AuthorsShortList IN (\"Abur et al.\", \"Feeney et al.\", \"Groon et al.\", \"Lewis and Neely\", \"Liu et al.\", \"Rosowski et al.\", \"Shahnaz et al.\", \"Shaver and Sun\", \"Sun et al.\", \"Voss and Allen\", \"Voss et al.\", \"Werner et al.\") AND Measurements.Frequency BETWEEN 0 and 8000\nGROUP BY Measurements.Identifier, Measurements.Frequency,  Measurements.Instrument;\n\n\ntable1 %&gt;%\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n  geom_line(aes(color = studies), se = FALSE) + \n  labs(\n    title = \"Mean absorbance from publications in WAI database\"\n  )\n\n\n\n\n\n\n\n\n\nSELECT\n  Subjects.Sex AS sex,\n  Subjects.Race AS race,\n  Subjects.Ethnicity AS ethnicity,\n  Subjects.Identifier,\n  Measurements.Frequency AS freq,\n  AVG(Measurements.Absorbance) AS mean_absorbance\nFROM Subjects\nJOIN Measurements ON Subjects.SubjectNumber = Measurements.SubjectNumber \nWHERE Subjects.Identifier = \"Aithal_2013\" AND Measurements.Identifier = \"Aithal_2013\"\nGROUP BY ethnicity, race, sex, freq;\n\n\ntable2 %&gt;%\n  ggplot(aes(x = freq, y = mean_absorbance, color = sex)) +\n  geom_line(se = FALSE) +\n  labs(\n    title = \"Mean absorbance from babies of different sexes\"\n  )\n\n\n\n\n\n\n\n\nUsing SQL queries, I filtered through and sorted data in a manner that allowed me to compare absorbance measurements across babies of different sexes with data from Aithal(2013)and copy figure 1 of Voss(2019). I did this with 2 SQL queries and joined different tables from the same database to produce both graphs. Following each query, I used ggplot to plot mean absorbance measurements alongside frequency.\n\n\nAithal et al. 2013. “Normative wideband reflectance measures in healthy neonates.” International Journal of Pediatric Otorhinolaryngology 77 (1). https://doi.org/10.1016/j.ijporl.2012.09.02\nVoss, SE. 2019. “Resource Review.” Ear and Hearing 40 (6). https://doi.org/10.1097/AUD.0000000000000790."
  },
  {
    "objectID": "Project 2/Netflix_Analysis.html",
    "href": "Project 2/Netflix_Analysis.html",
    "title": "Analysis of Netflix Horror Movies and TV Shows",
    "section": "",
    "text": "library(RTextTools) \nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nnetflix &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')"
  },
  {
    "objectID": "Project 2/Netflix_Analysis.html#netflix-horror-movies",
    "href": "Project 2/Netflix_Analysis.html#netflix-horror-movies",
    "title": "Analysis of Netflix Horror Movies and TV Shows",
    "section": "Netflix Horror Movies",
    "text": "Netflix Horror Movies\n\nhorror_subgenre &lt;- netflix %&gt;%\n  filter(str_detect(listed_in, \"Horror\"), type == \"Movie\") %&gt;%\n  mutate(listed_in = str_remove(listed_in, \", International Movies\")) %&gt;%\n  mutate(listed_in = str_remove(listed_in, \", Independent Movies\")) %&gt;%\n  mutate(listed_in = str_remove_all(listed_in, \"[,]\")) %&gt;%\n  mutate(listed_in = str_remove_all(listed_in, \"Movies\")) %&gt;%\n  mutate(listed_in = str_replace(listed_in, \"[:space:]&[:space:]\", \"&\")) %&gt;%\n  group_by(listed_in) %&gt;%\n  summarize(num = n()) %&gt;%\n  arrange(desc(num)) %&gt;%\n  mutate(listed_in = str_remove(listed_in, \"Horror\"))\n\n\nnum_thrillers &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Thrillers\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Thrillers = 'str_detect(listed_in, \\\"Thrillers\\\")') %&gt;%\n  filter(Thrillers == TRUE) %&gt;%\n  mutate(genre = \"Thrillers\") %&gt;%\n  select(-Thrillers)\n\nnum_comedies &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Comedies\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Comedies = 'str_detect(listed_in, \\\"Comedies\\\")') %&gt;%\n  filter(Comedies == TRUE) %&gt;%\n  mutate(genre = \"Comedies\") %&gt;%\n  select(-Comedies)\n\n\nnum_action_adventure &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Action/Adventure\"))%&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Action_Adventure = 'str_detect(listed_in, \\\"Action/Adventure\\\")') %&gt;%\n  filter(Action_Adventure == TRUE) %&gt;%\n  mutate(genre = \"Action&Adventure\") %&gt;%\n  select(-Action_Adventure)\n\nnum_scifi_fantasy &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Sci-Fi&Fantasy\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(SciFi_Fantasy = 'str_detect(listed_in, \\\"Sci-Fi&Fantasy\\\")') %&gt;%\n  filter(SciFi_Fantasy == TRUE) %&gt;%\n  mutate(genre = \"SciFi&Fantasy\") %&gt;%\n  select(-SciFi_Fantasy)\n\nnum_cult &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Cult\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Cult = 'str_detect(listed_in, \\\"Cult\\\")') %&gt;%\n  filter(Cult == TRUE) %&gt;%\n  mutate(genre = \"Cult\") %&gt;%\n  select(-Cult)\n\nnum_documentaries &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Documentaries\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Documentaries = 'str_detect(listed_in, \\\"Documentaries\\\")') %&gt;%\n  filter(Documentaries == TRUE)%&gt;%\n  mutate(genre = \"Documentaties\") %&gt;%\n  select(-Documentaries)\n\nnum_romantic &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Romantic\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Romantic = 'str_detect(listed_in, \\\"Romantic\\\")') %&gt;%\n  filter(Romantic == TRUE) %&gt;%\n  mutate(genre = \"Romantic\") %&gt;%\n  select(-Romantic)\n\nnum_horror &lt;- horror_subgenre %&gt;%\n  filter(listed_in == \" \") %&gt;%\n  mutate(listed_in = \"horror\") %&gt;%\n  rename(genre = listed_in)\n\nhorror_df &lt;- rbind(num_thrillers, num_comedies, num_action_adventure, num_scifi_fantasy, num_cult, num_documentaries, num_romantic, num_horror ) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(num))\nhorror_df\n\n  num         genre\n1 125        horror\n2 110     Thrillers\n3  38      Comedies\n4  19 SciFi&Fantasy\n5  12          Cult\n6   2 Documentaties\n7   2      Romantic\n\n\n\nggplot(horror_df, aes(x = genre, y = num)) +\n  geom_bar(stat = \"identity\", aes(fill = genre)) + labs(\n    title = \"Subgenres among netflix horror movies\",\n    x = \"Subgenre of horror movie\",\n    y = \"Count\"\n  ) + \n  scale_x_discrete(guide = guide_axis(n.dodge=3)) +\n  guides(fill=guide_legend(title=\"Subgenres\"))\n\n\n\n\n\n\n\n\nBased on this plot of the number of horror movies with specific subgenres, it seems most netflix horror movies either do not have a subgenre or have thriller as their subgenre. Other subgenres appear substantially less present among Netflix horror movies."
  },
  {
    "objectID": "Project 2/Netflix_Analysis.html#netflix-tv-shows",
    "href": "Project 2/Netflix_Analysis.html#netflix-tv-shows",
    "title": "Analysis of Netflix Horror Movies and TV Shows",
    "section": "Netflix TV Shows",
    "text": "Netflix TV Shows\n\ndata &lt;- netflix %&gt;% \n  filter(type == \"TV Show\") %&gt;%\n  mutate(num_seasons = as.numeric(str_extract(duration, \"\\\\d+\" )))\n\nggplot(data, aes(x = release_year, y = num_seasons)) +\n  geom_point() +\n  xlim(1970,2025)+\n  geom_smooth(se = FALSE) + \n  labs(\n    title = \"Relationship between release year and number of seasons of netflix tv show\",\n    x = \"Release year\",\n    y = \"Number of seasons\"\n  )\n\n\n\n\n\n\n\n\nBased on this plot, the number of seasons that a Netflix show has tends to be lower for shows released between 2005 and 2020 than for other shows. Due to the discreteness of values on the x-axis, multiple points are most likely overlapping. This explains why it appears the right side of the graph has less points below the line of best fit than above it even with the line trending downwards.\nData source: https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv"
  },
  {
    "objectID": "Beta_Simulation.html",
    "href": "Beta_Simulation.html",
    "title": "Using Normal Prediction Intervals on Symmetric Beta-distributed data",
    "section": "",
    "text": "Symmetric beta distributions and normal distributions are very similar to each other in shape. Due to this, in this project I examine how a 95% prediction interval that assumes data originates from a normal distribution would fare in terms of prediction accuracy and actual coverage probability of beta-distributed data. I define a prediction interval with “good” prediction accuracy as one that covers the actual mean of the data’s parent distribution, and a prediction interval with good coverage probability as one that covers approximately .95 of the beta distribution. All data is generated from beta distributions with mean .5."
  },
  {
    "objectID": "Beta_Simulation.html#introduction",
    "href": "Beta_Simulation.html#introduction",
    "title": "Using Normal Prediction Intervals on Symmetric Beta-distributed data",
    "section": "",
    "text": "Symmetric beta distributions and normal distributions are very similar to each other in shape. Due to this, in this project I examine how a 95% prediction interval that assumes data originates from a normal distribution would fare in terms of prediction accuracy and actual coverage probability of beta-distributed data. I define a prediction interval with “good” prediction accuracy as one that covers the actual mean of the data’s parent distribution, and a prediction interval with good coverage probability as one that covers approximately .95 of the beta distribution. All data is generated from beta distributions with mean .5."
  },
  {
    "objectID": "Beta_Simulation.html#simulations",
    "href": "Beta_Simulation.html#simulations",
    "title": "Using Normal Prediction Intervals on Symmetric Beta-distributed data",
    "section": "Simulations",
    "text": "Simulations\nFirst, I tidy up.\n\nlibrary(tidyverse)\n\nThen, I define a few functions that will help me in my task.\n\nPI &lt;- function(data, coverage_prob){ \n  #Generates a normal prediction interval with an intended coverage probability of coverage_prob based on a vector of numeric data\n  n &lt;- length(data)\n  lower_tscore &lt;- qt((1-coverage_prob)/2, df = n - 1)\n  upper_tscore &lt;- qt(((1-coverage_prob)/2) + coverage_prob, df = n - 1)\n  avg &lt;- mean(data)\n  stan_d &lt;- sd(data)\n  lower_bound &lt;- avg + lower_tscore*stan_d * sqrt(1 + (1/n))\n  upper_bound &lt;- avg + upper_tscore*stan_d * sqrt(1 + (1/n))\n  return(data.frame(PI_percentage = coverage_prob, lower = lower_bound, upper = upper_bound))\n}\n\nPI is a function that takes a vector of numeric data and produces a high-density prediction interval intended to cover the proportion of said data’s parent distribution given by coverage_prob. This prediction interval is created under the assumption that the data is normally distributed (although we know it is not). Let \\(s\\) be the sample standard deviation of our data, \\(n\\) be the size of our sample, \\(\\hat{p}\\) be the mean of our data, and \\(t_{p,n-1}\\) be the quantile value associated with the highest density coverage probability \\(p\\) in a \\(t\\) distribution with \\(n-1\\) degrees of freedom. If we assume that our data is normal, then the highest density prediction interval with coverage probability \\(p\\) is best predicted as\n\\[\\hat{p} \\pm t_{p,n-1}*s*\\sqrt{1+\\frac{1}{n}}\\] PI uses this formula to generate the bounds of its prediction interval.\n\none_beta_simulation &lt;- function(n, alpha, beta, pi_prop){\n  #Assesses prediction accuracy and actual coverage probability of a normal prediction interval when used on a vector of numeric data of size n. The numeric data is generated from a beta distribution with parameters alpha and beta.\n  \n  cover_df &lt;- PI(rbeta(n, alpha, beta), pi_prop)\n  \n  cover_prop &lt;- pbeta(cover_df[1, \"upper\"], alpha, beta) - pbeta(cover_df[1, \"lower\"], alpha, beta) #this is the proportion of the data's parent distribution that is actually covered by the normal prediction interval generated for said data.\n  \n  mean_in_interval &lt;- .5 &gt;= cover_df[1, \"lower\"] & .5 &lt;= cover_df[1,\"upper\"]\n  param_df &lt;- data.frame(cover = cover_prop, alpha = rep(alpha, nrow(cover_df)), beta = rep(beta, nrow(cover_df)), mean_in_interval = mean_in_interval)\n  df &lt;- cbind(cover_df, param_df)\n  return(df)\n}\n\none_beta_simulation randomly samples \\(n\\) data points from a beta distribution with parameters alpha and beta and calculates a normal prediction interval for said data using function PI. It then determines what proportion of the data’s parent beta distribution is covered by the prediction interval and checks if the mean of said distribution is covered by the prediction interval.\n\nbeta_sims_n &lt;- function(n){\n  #Iterates over a vector of possible alpha = beta values and applies one_beta_simulation to each possible value of alpha/beta. All simulations use data of sample size n.\n  df1 &lt;- map(parameters,\\(param) one_beta_simulation(n, param, param, pi) ) %&gt;%\n  list_rbind()\n  df2 &lt;- data.frame(n = rep(n, nrow(df1)))\n  df &lt;- cbind(df2, df1)\n  return(df)\n}\n\nbeta_sims_n maps over a vector of potential parameter values for a beta distribution with mean \\(.5\\) and uses one_beta_simulation per each iteration, with all iterations taking the same value for the data’s sample size. It randomly samples \\(n\\) points multiple times from different beta distribution with mean .5 and generates normal prediction intervals for each sample. It also determines the proportion of the each sample’s parent distribution that said intervals actually cover.\nTime to simulate over different parameter values and sample sizes!\n\nparameters &lt;- seq(5, 200, by = 2)\nn &lt;- 2:500\npi &lt;- .95\n\nbeta_df &lt;- map(n, \\(n) beta_sims_n(n)) %&gt;%\n  list_rbind()\n\nThis is a glimpse at the results of the simulations.\n\nrows &lt;- sample(1:nrow(beta_df), 10)\nmap(rows, \\(i) beta_df[i,]) %&gt;%\n  list_rbind()\n\n     n PI_percentage     lower     upper     cover alpha beta mean_in_interval\n1  456          0.95 0.4351501 0.5647684 0.9533053   117  117             TRUE\n2  136          0.95 0.4423668 0.5609925 0.9699839   167  167             TRUE\n3  274          0.95 0.4155896 0.5875906 0.9437346    61   61             TRUE\n4  182          0.95 0.4327099 0.5683226 0.9657378   121  121             TRUE\n5  432          0.95 0.4392460 0.5603486 0.9600007   143  143             TRUE\n6  128          0.95 0.4412504 0.5592070 0.9739499   177  177             TRUE\n7  359          0.95 0.4191253 0.5767552 0.9405113    71   71             TRUE\n8  479          0.95 0.2297765 0.7638826 0.9668517     7    7             TRUE\n9   91          0.95 0.3906936 0.6013691 0.9386560    39   39             TRUE\n10  86          0.95 0.4039962 0.6199326 0.9512507    43   43             TRUE\n\n\nThis is a random sample of rows from the actual dataset, which has 48902 rows and 8 columns. Each row corresponds to a simulated random sample of size n from a beta distribution with parameters alpha and beta. For each random sample, a normal prediction interval was generated with bounds “lower” and “upper”. “PI_percentage” refers to the intended coverage probability and “cover” refers to the actual coverage probability of said prediction interval over the beta distribution the data was generated from. “mean_in_interval” is a binary variable that states if the prediction interval covered the mean of the distribution."
  },
  {
    "objectID": "Beta_Simulation.html#insights",
    "href": "Beta_Simulation.html#insights",
    "title": "Using Normal Prediction Intervals on Symmetric Beta-distributed data",
    "section": "Insights",
    "text": "Insights\n\nn_means_df &lt;- beta_df %&gt;%\n  mutate(diff = cover - PI_percentage) %&gt;%\n  group_by(n) %&gt;%\n  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n())\navg &lt;- mean(n_means_df$mean)\n\n\nggplot(n_means_df, aes(x = n, y = mean)) + \n  geom_point() + \n  geom_hline(yintercept = 0, col = \"black\")  +\n  geom_hline(yintercept = avg, col = \"red\")  +\n  labs(\n    x = \"sample size\",\n    y = \"mean difference between actual and intended coverage\",\n    title = \"Figure 1\",\n    subtitle = \"difference between actual and intended coverage probability based on sample size\",\n  )\n\n\n\n\n\n\n\n\nFigure 1 graphs the mean difference between the actual coverage probability and intended coverage probability (calculated as: actual coverage probability - intended coverage probability) per each sample size from 2 to 500. A negative mean difference indicates the actual coverage probability tended to be less than the intended coverage probability, which is undesirable, while a positive mean difference indicates the actual cover probability tended to be more than the intended coverage probability. The mean difference between actual and intended coverage probability seems to converge to a value a little bit above 0 as the sample size increases, indicating that actual coverage probability tends to be more than the intended coverage and that this is more so as sample size increases.\nFor small sample sizes the mean difference between actual and intended coverage probability has a lot more variance around 0 than it does for larger sample sizes, which can be attributed to sampling variability. Overall, absolute differences in actual coverage probability and intended coverage probability don’t appear to extend beyond .02, indicating normal prediction intervals tend to have good coverage probability for symmetric, beta distributed data.\n\nn_means_df &lt;- beta_df %&gt;%\n  mutate(diff = cover - PI_percentage) %&gt;%\n  group_by(n) %&gt;%\n  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n()) %&gt;%\n  filter(n %in% 1:30)\n\n\nggplot(n_means_df, aes(x = n, y = mu_in_interval)) + \n  geom_point()  +\n  labs(\n    x = \"sample size\",\n    y = \"proportion of mu-inclusive prediction intervals\",\n    title = \"Figure 2\",\n    subtitle = \"proportion of mu-inclusive prediction intervals based on sample size\"\n  )\n\n\n\n\n\n\n\n\nFigure 2 plots the proportion of prediction intervals that cover mu = .5 per each sample size from 2 to 30. These points converge to 1 at a sample size of around 4, meaning for sample sizes of 4 or greater it is probable that all random samples with those samples sizes will produce normal-prediction intervals that cover the mean of the beta distribution said data originated from.\n\nns_of_interest &lt;- c(5, 10, 30, 50, 100)\nbeta_df_2 &lt;- filter(beta_df,n %in% ns_of_interest) %&gt;%\n  mutate(sample_size = as.factor(n))\n\n\nggplot(beta_df_2, aes(x = alpha, y = cover, color = sample_size)) +\n  geom_point() +\n  geom_smooth(aes(line = n ), se = FALSE) +\n  labs(\n    x = \"value of alpha and beta\",\n    y = \"actual coverage probability\",\n    title = \"Figure 3\",\n    subtitle = \"actual coverage probability based on parameter values\"\n  )\n\n\n\n\n\n\n\n\nFigure 3 plots the actual coverage probability per each value of alpha and beta. Points and lines are colored based on sample size, which are described in the legend. Alpha and beta have the same values, by the way. All the lines of best fit seem close to being horizontal, meaning that there most likely is not a relationship between the values of alpha/beta and the actual coverage probability of the data’s prediction interval. Based on the large overlap between lines, normal prediction intervals based on small amounts of data don’t seem more likely to deviate from the intended coverage probability of the data’s parent symmetric beta distribution, although they do have larger amounts of variability around their mean coverage probability."
  },
  {
    "objectID": "Beta_Simulation.html#conclusion",
    "href": "Beta_Simulation.html#conclusion",
    "title": "Using Normal Prediction Intervals on Symmetric Beta-distributed data",
    "section": "Conclusion",
    "text": "Conclusion\nBy generating data from symmetric beta distributions with mu = .5 and making normal prediction intervals based on this data, I was able to assess the predictive accuracy and actual coverage probability of normal prediction intervals when applied to symmetric-beta data. Even given small sample sizes, normal prediction intervals seem to have good predictive accuracy and good coverage probability."
  },
  {
    "objectID": "Netflix_Analysis.html",
    "href": "Netflix_Analysis.html",
    "title": "Netflix",
    "section": "",
    "text": "library(RTextTools) \nlibrary(tidyverse)\nlibrary(tidytuesdayR)"
  },
  {
    "objectID": "Netflix_Analysis.html#netflix-horror-movies",
    "href": "Netflix_Analysis.html#netflix-horror-movies",
    "title": "Netflix",
    "section": "Netflix Horror Movies",
    "text": "Netflix Horror Movies\nIn this analysis of Netflix horror movies, I look at all movies with horror listed as a genre and see what other genres are also listed for said movies. I then create a bar plot comparing the number of other genres listed among Netflix horror movies.\nFirst, I filter for movies with horror listed as one of its genres and clean the variable listing movie genres so that it only includes genres that aren’t horror. This is all saved in the “horror_subgenre” object.\n\nhorror_subgenre &lt;- netflix %&gt;%\n  filter(str_detect(listed_in, \"Horror\"), type == \"Movie\") %&gt;%\n  mutate(listed_in = str_remove(listed_in, \", International Movies\")) %&gt;%\n  mutate(listed_in = str_remove(listed_in, \", Independent Movies\")) %&gt;%\n  mutate(listed_in = str_remove_all(listed_in, \"[,]\")) %&gt;%\n  mutate(listed_in = str_remove_all(listed_in, \"Movies\")) %&gt;%\n  mutate(listed_in = str_replace(listed_in, \"[:space:]&[:space:]\", \"&\")) %&gt;%\n  group_by(listed_in) %&gt;%\n  summarize(num = n()) %&gt;%\n  arrange(desc(num)) %&gt;%\n  mutate(listed_in = str_remove(listed_in, \"Horror\"))\n\nThen, I count the number of movies within “horror_subgenre” that list a specific secondary genre, with the secondary genres I consider being the following: thriller, comedy, romance, action/adventure, science fiction and fantasy, cult, documentary, and none. None means that the movie only has horror as its genre.\n\nnum_thrillers &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Thrillers\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Thrillers = 'str_detect(listed_in, \\\"Thrillers\\\")') %&gt;%\n  filter(Thrillers == TRUE) %&gt;%\n  mutate(genre = \"Thrillers\") %&gt;%\n  select(-Thrillers)\n\nnum_comedies &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Comedies\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Comedies = 'str_detect(listed_in, \\\"Comedies\\\")') %&gt;%\n  filter(Comedies == TRUE) %&gt;%\n  mutate(genre = \"Comedies\") %&gt;%\n  select(-Comedies)\n\n\nnum_action_adventure &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Action/Adventure\"))%&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Action_Adventure = 'str_detect(listed_in, \\\"Action/Adventure\\\")') %&gt;%\n  filter(Action_Adventure == TRUE) %&gt;%\n  mutate(genre = \"Action&Adventure\") %&gt;%\n  select(-Action_Adventure)\n\nnum_scifi_fantasy &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Sci-Fi&Fantasy\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(SciFi_Fantasy = 'str_detect(listed_in, \\\"Sci-Fi&Fantasy\\\")') %&gt;%\n  filter(SciFi_Fantasy == TRUE) %&gt;%\n  mutate(genre = \"SciFi&Fantasy\") %&gt;%\n  select(-SciFi_Fantasy)\n\nnum_cult &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Cult\")) %&gt;% \n  summarize(num = sum(num)) %&gt;%\n  rename(Cult = 'str_detect(listed_in, \\\"Cult\\\")') %&gt;%\n  filter(Cult == TRUE) %&gt;%\n  mutate(genre = \"Cult\") %&gt;%\n  select(-Cult)\n\nnum_documentaries &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Documentaries\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Documentaries = 'str_detect(listed_in, \\\"Documentaries\\\")') %&gt;%\n  filter(Documentaries == TRUE)%&gt;%\n  mutate(genre = \"Documentaries\") %&gt;%\n  select(-Documentaries)\n\nnum_romantic &lt;- horror_subgenre %&gt;%\n  group_by(str_detect(listed_in,\"Romantic\")) %&gt;%\n  summarize(num = sum(num)) %&gt;%\n  rename(Romantic = 'str_detect(listed_in, \\\"Romantic\\\")') %&gt;%\n  filter(Romantic == TRUE) %&gt;%\n  mutate(genre = \"Romantic\") %&gt;%\n  select(-Romantic)\n\nnum_horror &lt;- horror_subgenre %&gt;%\n  filter(listed_in == \" \") %&gt;%\n  mutate(listed_in = \"none\") %&gt;%\n  rename(genre = listed_in) %&gt;%\n  select(genre, num)\n\nhorror_df &lt;- rbind(num_thrillers, num_comedies, num_action_adventure, num_scifi_fantasy, num_cult, num_documentaries, num_romantic, num_horror ) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(num)) %&gt;%\n  select(genre, num)\nhorror_df\n\n          genre num\n1          none 125\n2     Thrillers 110\n3      Comedies  38\n4 SciFi&Fantasy  19\n5          Cult  12\n6 Documentaries   2\n7      Romantic   2\n\n\n\nggplot(horror_df, aes(x = genre, y = num)) +\n  geom_bar(stat = \"identity\", aes(fill = genre)) + labs(\n    title = \"Subgenres among netflix horror movies\",\n    x = \"Subgenre of horror movie\",\n    y = \"Count\"\n  ) + \n  scale_x_discrete(guide = guide_axis(n.dodge=3)) +\n  guides(fill=guide_legend(title=\"Subgenres\"))\n\n\n\n\n\n\n\n\nBased on this plot of the number of horror movies with specific secondary genres, it seems most netflix horror movies either do not have a secondary genre or have thriller as their secondary genre. Other secondary genres appear less present among Netflix horror movies."
  },
  {
    "objectID": "Netflix_Analysis.html#netflix-tv-shows",
    "href": "Netflix_Analysis.html#netflix-tv-shows",
    "title": "Netflix",
    "section": "Netflix TV Shows",
    "text": "Netflix TV Shows\nIn this analysis of Netflix tv shows, I look at the number of seasons present in Netflix shows released in different years. A season in a Netflix show is a set of episodes pertaining to said show that was greenlit to be released in a certain time period. A show having multiple seasons means different sets of episodes were allowed to be released at different periods of time.\n\ndata &lt;- netflix %&gt;% \n  filter(type == \"TV Show\") %&gt;%\n  mutate(num_seasons = as.numeric(str_extract(duration, \"\\\\d+\" )))\n\nggplot(data, aes(x = release_year, y = num_seasons)) +\n  geom_point() +\n  xlim(1970,2025)+\n  geom_smooth(se = FALSE) + \n  labs(\n    title = \"Relationship between release year and number of seasons of netflix tv show\",\n    x = \"Release year\",\n    y = \"Number of seasons\"\n  )\n\n\n\n\n\n\n\n\nBased on this plot, the number of seasons that a Netflix show has tends to be lower for shows released between 2005 and 2020 than for other shows. Due to the discreteness of values on the x-axis, multiple points are most likely overlapping. This explains why it appears the right side of the graph has less points below the line of best fit than above it even with the line trending downwards.\nData source here."
  },
  {
    "objectID": "Presentation.html#netflix-horror-movies",
    "href": "Presentation.html#netflix-horror-movies",
    "title": "Presentation",
    "section": "Netflix Horror Movies",
    "text": "Netflix Horror Movies\n\nhorror_df\n\n  num         genre\n1 125        Horror\n2 110     Thrillers\n3  38      Comedies\n4  19 SciFi&Fantasy\n5  12          Cult\n6   2 Documentaties\n7   2      Romantic"
  },
  {
    "objectID": "Presentation.html#diversity-of-horror",
    "href": "Presentation.html#diversity-of-horror",
    "title": "Presentation",
    "section": "Diversity of Horror",
    "text": "Diversity of Horror"
  },
  {
    "objectID": "Presentation.html#netflix-horror-movies-1",
    "href": "Presentation.html#netflix-horror-movies-1",
    "title": "Presentation",
    "section": "Netflix Horror Movies",
    "text": "Netflix Horror Movies"
  },
  {
    "objectID": "Presentation.html#beta-vs-normal-distribution",
    "href": "Presentation.html#beta-vs-normal-distribution",
    "title": "Presentation",
    "section": "Beta vs Normal Distribution",
    "text": "Beta vs Normal Distribution"
  },
  {
    "objectID": "Presentation.html#simulations",
    "href": "Presentation.html#simulations",
    "title": "Presentation",
    "section": "Simulations",
    "text": "Simulations\n\n\n     n PI_percentage lower upper cover alpha beta mean_in_interval\n1  164      3.141593   NaN   NaN   NaN   123  123               NA\n2   67      3.141593   NaN   NaN   NaN    17   17               NA\n3   78      3.141593   NaN   NaN   NaN   113  113               NA\n4  409      3.141593   NaN   NaN   NaN   195  195               NA\n5  337      3.141593   NaN   NaN   NaN    67   67               NA\n6   31      3.141593   NaN   NaN   NaN   151  151               NA\n7  332      3.141593   NaN   NaN   NaN    65   65               NA\n8  200      3.141593   NaN   NaN   NaN   187  187               NA\n9   88      3.141593   NaN   NaN   NaN    91   91               NA\n10 456      3.141593   NaN   NaN   NaN    55   55               NA"
  },
  {
    "objectID": "Presentation.html#results",
    "href": "Presentation.html#results",
    "title": "Presentation",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "Presentation.html#function-1-pi-interval-calculation",
    "href": "Presentation.html#function-1-pi-interval-calculation",
    "title": "Presentation",
    "section": "Function 1: PI Interval Calculation",
    "text": "Function 1: PI Interval Calculation\n\nPI &lt;- function(data, coverage_prob){ \n  #Generates a normal prediction interval with an intended coverage probability of coverage_prob based on a vector of numeric data\n  n &lt;- length(data)\n  lower_tscore &lt;- qt((1-coverage_prob)/2, df = n - 1)\n  upper_tscore &lt;- qt(((1-coverage_prob)/2) + coverage_prob, df = n - 1)\n  avg &lt;- mean(data)\n  stan_d &lt;- sd(data)\n  lower_bound &lt;- avg + lower_tscore*stan_d * sqrt(1 + (1/n))\n  upper_bound &lt;- avg + upper_tscore*stan_d * sqrt(1 + (1/n))\n  return(data.frame(PI_percentage = coverage_prob, lower = lower_bound, upper = upper_bound))\n}"
  },
  {
    "objectID": "Presentation.html#function-2-one-simulation-of-beta-generated-data",
    "href": "Presentation.html#function-2-one-simulation-of-beta-generated-data",
    "title": "Presentation",
    "section": "Function 2: One simulation of beta-generated data",
    "text": "Function 2: One simulation of beta-generated data\n\none_beta_simulation &lt;- function(n, alpha, beta, pi_prop){\n  #Assesses prediction accuracy and actual coverage probability of a normal prediction interval when used on a vector of numeric data of size n. The numeric data is generated from a beta distribution with parameters alpha and beta.\n  \n  cover_df &lt;- PI(rbeta(n, alpha, beta), pi_prop)\n  \n  cover_prop &lt;- pbeta(cover_df[1, \"upper\"], alpha, beta) - pbeta(cover_df[1, \"lower\"], alpha, beta) #this is the proportion of the data's parent distribution that is actually covered by the normal prediction interval generated for said data.\n  \n  mean_in_interval &lt;- .5 &gt;= cover_df[1, \"lower\"] & .5 &lt;= cover_df[1,\"upper\"]\n  param_df &lt;- data.frame(cover = cover_prop, alpha = rep(alpha, nrow(cover_df)), beta = rep(beta, nrow(cover_df)), mean_in_interval = mean_in_interval)\n  df &lt;- cbind(cover_df, param_df)\n  return(df)\n}"
  },
  {
    "objectID": "Presentation.html#function-3-multiple-beta-simulations",
    "href": "Presentation.html#function-3-multiple-beta-simulations",
    "title": "Presentation",
    "section": "Function 3: Multiple Beta simulations",
    "text": "Function 3: Multiple Beta simulations\n\nbeta_sims_n &lt;- function(n){\n  #Iterates over a vector of possible alpha = beta values and applies one_beta_simulation to each possible value of alpha/beta. All simulations use data of sample size n.\n  df1 &lt;- map(parameters,\\(param) one_beta_simulation(n, param, param, pi) ) %&gt;%\n  list_rbind()\n  df2 &lt;- data.frame(n = rep(n, nrow(df1)))\n  df &lt;- cbind(df2, df1)\n  return(df)\n}"
  },
  {
    "objectID": "Netflix_Analysis.html#introduction",
    "href": "Netflix_Analysis.html#introduction",
    "title": "Netflix",
    "section": "Introduction",
    "text": "Introduction\nThis project consists of me analyzing data about Netflix shows and movies, with said analysis focusing on two aspects of these: horror movies and number of seasons in said shows. The dataset used for this analysis can be found here. According to the Tidy Tuesday github page I found it in, it originates from Kaggle and was gathered by Shival Bansal. Here is what he says about the data:\n\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n\nHere is the data dictionary (sourced from here):\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nshow_id\ncharacter\nUnique ID for every Movie / Tv Show\n\n\ntype\ncharacter\nIdentifier - A Movie or TV Show\n\n\ntitle\ncharacter\nTitle of the Movie / Tv Show\n\n\ndirector\ncharacter\nDirector of the Movie/Show\n\n\ncast\ncharacter\nActors involved in the movie / show\n\n\ncountry\ncharacter\nCountry where the movie / show was produced\n\n\ndate_added\ncharacter\nDate it was added on Netflix\n\n\nrelease_year\ndouble\nActual Release year of the movie / show\n\n\nrating\ncharacter\nTV Rating of the movie / show\n\n\nduration\ncharacter\nTotal Duration - in minutes or number of seasons\n\n\nlisted_in\ncharacter\nGenre\n\n\ndescription\ncharacter\nSummary description of the film/show\n\n\n\n\nnetflix &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')"
  },
  {
    "objectID": "Netflix_Analysis.html#data",
    "href": "Netflix_Analysis.html#data",
    "title": "Netflix Analysis",
    "section": "Data",
    "text": "Data\nThis project consists of me analyzing data about Netflix shows and movies, with said analysis focusing on two aspects of these: horror movies and number of seasons in said shows. The dataset used for this analysis can be found here. According to the Tidy Tuesday github page I found it in, it originates from Kaggle and was gathered by Shival Bansal. Here is what he says about the data:\n\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n\nHere is the data dictionary (sourced from here):\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nshow_id\ncharacter\nUnique ID for every Movie / Tv Show\n\n\ntype\ncharacter\nIdentifier - A Movie or TV Show\n\n\ntitle\ncharacter\nTitle of the Movie / Tv Show\n\n\ndirector\ncharacter\nDirector of the Movie/Show\n\n\ncast\ncharacter\nActors involved in the movie / show\n\n\ncountry\ncharacter\nCountry where the movie / show was produced\n\n\ndate_added\ncharacter\nDate it was added on Netflix\n\n\nrelease_year\ndouble\nActual Release year of the movie / show\n\n\nrating\ncharacter\nTV Rating of the movie / show\n\n\nduration\ncharacter\nTotal Duration - in minutes or number of seasons\n\n\nlisted_in\ncharacter\nGenre\n\n\ndescription\ncharacter\nSummary description of the film/show\n\n\n\n\nnetflix &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')"
  },
  {
    "objectID": "SQL_Proj.html#analysis",
    "href": "SQL_Proj.html#analysis",
    "title": "Wideband Acoustic Immittance Data Visualization",
    "section": "",
    "text": "I plan to query data from the Wideband Acoustic Immittance Database, which is a repository of auditory measurements from different people. From this data, two graphs will be generated. One graph will replicate Figure 1 from Voss(2019) and the other will compare mean absorbance measurements between babies of different sexes over different frequencies using data from Aithal et al.(2013).\nThis is the abstract of Aithal et al.(2013):\n\nPresently, normative wideband reflectance data are available for neonates who have passed a distortion product otoacoustic emission test. However, passing the distortion product otoacoustic emission test alone does not ensure normal middle ear function. The objective of this study was to establish normative wideband reflectance data in healthy neonates with normal middle ear function, as justified by passing a battery of tests.\n\nVoss(2019) is a summarization of different studies analyzing auditory measurements among people.\n\nlibrary(RMariaDB)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n #collect(Measurements)\n\n\nSELECT Measurements.Identifier,\n       Measurements.Frequency,\n       AVG(Absorbance) AS mean_absorbance,\n       CONCAT(PI_Info.AuthorsShortList,\" (\", PI_Info.Year, \")\",\" N=\", COUNT(DISTINCT Measurements.SubjectNumber) ,\"; \",Measurements.Instrument) AS studies\nFROM Measurements\nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nWHERE PI_Info.AuthorsShortList IN (\"Abur et al.\", \"Feeney et al.\", \"Groon et al.\", \"Lewis and Neely\", \"Liu et al.\", \"Rosowski et al.\", \"Shahnaz et al.\", \"Shaver and Sun\", \"Sun et al.\", \"Voss and Allen\", \"Voss et al.\", \"Werner et al.\") AND Measurements.Frequency BETWEEN 0 and 8000\nGROUP BY Measurements.Identifier, Measurements.Frequency,  Measurements.Instrument;\n\n\ntable1 %&gt;%\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n  geom_line(aes(color = studies), se = FALSE) + \n  labs(\n    title = \"Mean absorbance from publications in WAI database\"\n  )\n\n\n\n\n\n\n\n\n\nSELECT\n  Subjects.Sex AS sex,\n  Subjects.Race AS race,\n  Subjects.Ethnicity AS ethnicity,\n  Subjects.Identifier,\n  Measurements.Frequency AS freq,\n  AVG(Measurements.Absorbance) AS mean_absorbance\nFROM Subjects\nJOIN Measurements ON Subjects.SubjectNumber = Measurements.SubjectNumber \nWHERE Subjects.Identifier = \"Aithal_2013\" AND Measurements.Identifier = \"Aithal_2013\"\nGROUP BY ethnicity, race, sex, freq;\n\n\ntable2 %&gt;%\n  ggplot(aes(x = freq, y = mean_absorbance, color = sex)) +\n  geom_line(se = FALSE) +\n  labs(\n    title = \"Mean absorbance from babies of different sexes\"\n  )\n\n\n\n\n\n\n\n\nUsing SQL queries, I filtered through and sorted data in a manner that allowed me to compare absorbance measurements across babies of different sexes with data from Aithal(2013)and copy figure 1 of Voss(2019). I did this with 2 SQL queries and joined different tables from the same database to produce both graphs. Following each query, I used ggplot to plot mean absorbance measurements alongside frequency.\n\n\nAithal et al. 2013. “Normative wideband reflectance measures in healthy neonates.” International Journal of Pediatric Otorhinolaryngology 77 (1). https://doi.org/10.1016/j.ijporl.2012.09.02\nVoss, SE. 2019. “Resource Review.” Ear and Hearing 40 (6). https://doi.org/10.1097/AUD.0000000000000790."
  },
  {
    "objectID": "SQL_Proj.html#references",
    "href": "SQL_Proj.html#references",
    "title": "Wideband Acoustic Immittance Data Visualization",
    "section": "References",
    "text": "References\nAithal et al. 2013. “Normative wideband reflectance measures in healthy neonates.” International Journal of Pediatric Otorhinolaryngology 77 (1). https://doi.org/10.1016/j.ijporl.2012.09.02\nVoss, SE. 2019. “Resource Review.” Ear and Hearing 40 (6). https://doi.org/10.1097/AUD.0000000000000790."
  },
  {
    "objectID": "Bayesian_Optimization_Bernoulli.html",
    "href": "Bayesian_Optimization_Bernoulli.html",
    "title": "Bayesian Optimization with Bernoulli Data",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "Bayesian_Optimization_Bernoulli.html#introduction",
    "href": "Bayesian_Optimization_Bernoulli.html#introduction",
    "title": "Bayesian Optimization with Bernoulli Data",
    "section": "Introduction",
    "text": "Introduction\n“Taking the Human Out of the Loop: A Review of Bayesian Optimization” is a paper by Shahriari et al. that highlights the concepts and methods underpinning Bayesian Optimization and its applications. It first delves into the general concepts and methods behind Bayesian Optimization before delving into specific implementations of it for specific models. In general, Bayesian Optimization is a sequential method for estimating the optimal parameters of a model. Of interest are some unknown objective function, \\(f\\), and some \\(x*\\) such that $x* = $arg max \\(f(x)\\). \\(x\\) is an element of a parameter space \\(X\\) of interest. We also consider an observation \\(y\\) that is obtained from evaluating \\(f(x)\\). It is assumed to have been produced via a stochastic process involving \\(f(x)\\) in which \\(E(y|f(x)) = f(x)\\).\nIn Bayesian optimization, one initially starts off with a prior describing beliefs over what \\(f\\) might be and is interested in gaining a posterior that more accurately describes what \\(f\\) might be via sequential updates. In order to update the posterior, an acquisition function \\(\\alpha\\) is used at the \\(n\\)th iteration to find a value \\(x_{n+1}\\) that that maximizes it. At this same iteration, \\(x_{n+1}\\) is then used to query a objective function \\(f_n\\), the function most likely to be \\(f\\) based on the version of the posterior at the \\(n\\)th iteration, for an observation \\(y_n\\). Based on this new observation, the posterior is updated and used to determine \\(f_{n+1}\\).\nBayesian optimization can be used in a variety of settings and can be applied to both parametric and nonparametric models. This project will consider the parametric Bernoulli setting, in which a beta prior and a binomial likelihood are used. More specifically, it will implement the algorithm of Thompson Sampling for Beta Bernoulli Bandit as defined in section A of section II of Shahriari et al.(2016)."
  },
  {
    "objectID": "Bayesian_Optimization_Bernoulli.html#thompson-sampling-for-beta-bernoulli-bandit",
    "href": "Bayesian_Optimization_Bernoulli.html#thompson-sampling-for-beta-bernoulli-bandit",
    "title": "Bayesian Optimization with Bernoulli Data",
    "section": "Thompson Sampling for Beta Bernoulli Bandit",
    "text": "Thompson Sampling for Beta Bernoulli Bandit\nSuppose we have \\(k\\) Bernoulli parameters of interest corresponding to \\(k\\) objects of interest. Define an object of interest as \\(a \\in 1,2,...k\\). A bernoulli observation \\(y_i \\in\\){\\(0,1\\)} has a mean \\(f(a_i)\\) if observed repeatedly and given it was based on object \\(a_i\\). Let the Bernoulli parameters corresponding to each object be \\(\\theta_1,\\theta_2,...\\theta_k\\) and consider the vector \\(\\theta \\in \\mathbb{R^k}\\) which has all \\(k\\) Bernoulli parameters as scalars. Then a beta prior for \\(\\theta\\) can be defined as \\[p(\\theta|\\alpha,\\beta)= \\prod_{i=1}^{k} beta(\\theta_i | \\alpha_i, \\beta_i)\\]\nin which \\(beta(\\theta_i | \\alpha_i, \\beta_i)\\) is the beta prior for parameter \\(\\theta_i\\). Now let \\(n_{i,1}\\) be the number of successes attributed to \\(a_i\\) and \\(n_{i,0}\\) be the number of failures attributed to \\(a_i\\). Then given data \\(D\\) we can define a posterior for \\(\\theta\\) as \\[p(\\theta|D) = \\prod_{i=1}^{k} beta(\\theta_i | \\alpha_i + n_{i,1}, \\beta_i + n_{i,0} )\\]\nAfter \\(n-1\\) observations, Thomas Sampling can be considered a strategy for determining which object \\(a_n\\) observation \\(y_{n}\\) will be based on. The acquisition function in this setting is our posterior at the \\(n\\)th iteration, and we wish to choose the object \\(a_i\\) that produces the maximum draw from said posterior. We then pull an observation corresponding to \\(a_i\\) and use this observation to update the posterior. The exact algorithm is titled “Algorithm 2” in Shahriari et al.(2016)."
  },
  {
    "objectID": "Bayesian_Optimization_Bernoulli.html#references",
    "href": "Bayesian_Optimization_Bernoulli.html#references",
    "title": "Bayesian Optimization with Bernoulli Data",
    "section": "References",
    "text": "References\nShahriari et al. 2016. “Taking the Human Out of the Loop: A Review of Bayesian Optimization.” Proceedings of the IEEE 104 (1). https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7352306&tag=1"
  },
  {
    "objectID": "Bayesian_Optimization_Bernoulli.html#simulation",
    "href": "Bayesian_Optimization_Bernoulli.html#simulation",
    "title": "Bayesian Optimization with Bernoulli Data",
    "section": "Simulation",
    "text": "Simulation\nI will consider the situation in which \\(k=3\\) and I have no prior assumptions about what any \\(\\theta_i\\) might be. As such, each \\(\\theta_i\\) will have a prior of \\(beta(1,1)\\) which means \\(\\theta\\) will also have a prior of \\(beta(1,1)\\).\n\n#generating data from binomial distributions with with p = .81, p = .49,\n#and p = .37\none_data &lt;- rbinom(100,1,.81) \ntwo_data &lt;- rbinom(100,1,.49)\nthree_data &lt;- rbinom(100,1,.37)\ndata &lt;- list(one_data, two_data, three_data)\nn &lt;- length(one_data) + length(two_data) + length(three_data)\n\n#defining prior parameters\nalpha &lt;- rep(1,3)\nbeta &lt;- rep(1,3)\n\nith &lt;- c(0) #initializing sequence of indexes that indicate which distribution \n#produced the maximum draw at the ith iteration.\n\ndf &lt;- data.frame()\n\n\nfor (i in 1:100){ #Repeating Thomas Sampling 100 times \ndata &lt;- list(one_data, two_data, three_data)\n  for (i in 1:100){ #100 iterations\n    draws &lt;- c()\n    for (j in 1:3){\n      draws &lt;- c(draws, rbeta(1,alpha[j], beta[j]))\n    } \n    index &lt;- match(max(draws),draws) #identifying parameter which produces maximum draw\n    ith &lt;- c(ith, index) \n    draw_i &lt;- sample(data[[index]], size = 1, replace = FALSE) #observing y_i based on this draw\n    \n    #updating posterior based on y_i\n    if ((draw_i == 0) & ith[i+1] != ith[1]){\n      beta[index] &lt;- beta[index] + 1\n    }\n    else if ((draw_i == 1) & ith[i+1] != ith[1]){\n      alpha[index] &lt;- alpha[index] + 1\n    }\n  }\n    means &lt;- alpha/(alpha+beta)\n    df &lt;- as.data.frame(rbind(df,means))\n    \n    #resetting TS priors\n    alpha &lt;- rep(1,3)\n    beta &lt;- rep(1,3)\n}\n\nmean1 &lt;- mean(df[,1])\nmean2 &lt;- mean(df[,2])\nmean3 &lt;- mean(df[,3])\nactual_parameters &lt;- c(.81,.49,.37)\nTS_mean_parameters &lt;- c(mean1,mean2,mean3)\nTS_standard_deviation &lt;- c(sd(df[,1]),sd(df[,2]),sd(df[,3]))\nas.data.frame(cbind(TS_mean_parameters, TS_standard_deviation, actual_parameters))\n\n  TS_mean_parameters TS_standard_deviation actual_parameters\n1          0.7767256            0.04527662              0.81\n2          0.4221042            0.13301764              0.49\n3          0.3611869            0.11051629              0.37\n\n\nI used Thomas Sampling 100 times, with each time involving 100 iterations. “TS_parameters” displays the means of all the values most likely to be a specific parameter. Each of these values was determined by both the posteriors produced at the 100th iteration of every Thomas sample and the squared error loss function. “TS_standard_deviation” displays the sample standard deviation of all these values. The standard deviations would most likely have lower absolute values if informative priors had been used. They were quite large for estimations of \\(p = .49\\) and \\(p=.37\\), which would be problematic if only one Thomas Sample with \\(100\\) observations had been used. Thomas Sampling at this sample size might result in more stable estimates with informative priors."
  },
  {
    "objectID": "Pets_SriLanka.html",
    "href": "Pets_SriLanka.html",
    "title": "Pet Ownership in Sri Lanka",
    "section": "",
    "text": "Pet ownership has been extremely common in human society for a significant portion of history. Within the last century, rates of pet ownership have increased as motives for pet ownership have shifted from practical to those concerning emotional well-being and entertainment (Konstantinova). Looking at ways to measure their overall perceived benefit can provide insight into the way different factors influence a pet’s perceived value. The cost of a pet is commonly understood as being one such way to measure a pet’s emotional, physical, and economic value, meaning that factors influencing a pet’s cost can provide insight into determinants of pet value and the nuances to their large importance in society. Two possible factors influencing pet value include the type of pet that is owned and the location where the pet originates from.\nThis analysis will measure the effect of these factors on pet value within the context of specific cities in Sri Lanka. Research into pet ownership trends in Anuradhapura, Sri Lanka suggests certain pets might be more valued than others based on their increased prevalence among pet owners (Rathish). Using two-way Analysis of Variance (ANOVA), the type of pet will be evaluated alongside whether a pet resides in either Gampaha, Kandy, or Colombo, which are cities in Sri Lanka, to determine if either influences the cost of a pet. A model based on factor effects will then be developed in accordance to the results of this analysis. In order to evaluate the effect of these factors on pet prices, section two of this paper will discuss the data used and its appropriateness for analysis, section three will discuss the usage of two-way ANOVA to evaluate these factors and the implications of its results, and section four will discuss important points to take away from this analysis and potential future research directions."
  },
  {
    "objectID": "Pets_SriLanka.html#introduction",
    "href": "Pets_SriLanka.html#introduction",
    "title": "Pet Ownership in Sri Lanka",
    "section": "",
    "text": "Pet ownership has been extremely common in human society for a significant portion of history. Within the last century, rates of pet ownership have increased as motives for pet ownership have shifted from practical to those concerning emotional well-being and entertainment (Konstantinova). Looking at ways to measure their overall perceived benefit can provide insight into the way different factors influence a pet’s perceived value. The cost of a pet is commonly understood as being one such way to measure a pet’s emotional, physical, and economic value, meaning that factors influencing a pet’s cost can provide insight into determinants of pet value and the nuances to their large importance in society. Two possible factors influencing pet value include the type of pet that is owned and the location where the pet originates from.\nThis analysis will measure the effect of these factors on pet value within the context of specific cities in Sri Lanka. Research into pet ownership trends in Anuradhapura, Sri Lanka suggests certain pets might be more valued than others based on their increased prevalence among pet owners (Rathish). Using two-way Analysis of Variance (ANOVA), the type of pet will be evaluated alongside whether a pet resides in either Gampaha, Kandy, or Colombo, which are cities in Sri Lanka, to determine if either influences the cost of a pet. A model based on factor effects will then be developed in accordance to the results of this analysis. In order to evaluate the effect of these factors on pet prices, section two of this paper will discuss the data used and its appropriateness for analysis, section three will discuss the usage of two-way ANOVA to evaluate these factors and the implications of its results, and section four will discuss important points to take away from this analysis and potential future research directions."
  },
  {
    "objectID": "Pets_SriLanka.html#methods",
    "href": "Pets_SriLanka.html#methods",
    "title": "Pet Ownership in Sri Lanka",
    "section": "Methods",
    "text": "Methods\n\ndata &lt;- read.csv(\"~/Stats Work/Data Science HW/shirleytor.github.io/animal_prices.csv\") %&gt;%\n  mutate(Price_in_Rupees=as.numeric(str_remove_all(Price, \"[,Rs]\")))\nview(data)\nunique&lt;-unique(data[,5])\n\nThe data used for this analysis originates from Ikman.lk, a popular website for buying pets in Sri Lanka. It was collected through a script that compiles data from posts about pet sales (Jayawardena). The dataset itself has eleven variables, with one of them consisting of a url identifier for the post it pertains to. For the purpose of this analysis, only three variables are being examined: pet price, which is a numeric continuous variable, type of animal being sold, which is a categorical variable, and city where the animal is being sold, which is also a categorical variable. Pet price is measured in Rupees, Sri Lanka’s national currency, and is the response variable in this analysis. Type of pet being sold is an explanatory variable that consists of the following pet types: dog, cat, fish, and bird. City where the pet is being sold is also an explanatory variable that consists of the cities of Gampaha, Kandy, and Colombo. In order for all sample groups in this analysis to be balanced, they all have the same sample size of 10 different pet prices. The total sample size across all sample groups is 120 different pet prices. The sample groups were formed by using R to filter for pet prices corresponding to a specific pet type and Sri Lankan city-district. Two random districts corresponding to a specific city were chosen, and the first five prices related to a specific pet from each district were chosen to be part of the city sample. For instance, the first five bird prices from Kadawatha, Gampaha and the first five bird prices from Kiribathgoda, Gampaha were chosen to form the sample corresponding to bird prices in Gampaha. Since the data is ordered randomly and the districts were chosen randomly, there is no bias involved with the selection of the data. Each of these samples is thus random and can be used for inference. Due to the small sample size per sample group, it is unlikely they reflect the actual population of pet prices corresponding to a specific animal in a specific city. The sample size should still be large enough to determine if either factor affects pet price, however.\nData cleaning that samples from city-pet categories with 5 or more observations is performed with the code below.\n\nmore_than_100 = c() \nfor (each in unique){\n  if (nrow(dplyr::filter(data,Location==each)) &gt; 1){\n    more_than_100 = c(more_than_100,each)\n  }\n}\n\ncities = c()\nfor (each in more_than_100){\n  if (nrow(dplyr::filter(data,Location==each, Animal_Type==\"Dog\")) &gt; 5 & nrow(dplyr::filter(data,Location==each, Animal_Type==\"Cat\")) &gt; 5 & nrow(dplyr::filter(data,Location==each, Animal_Type==\"Fish\")) &gt; 5 & nrow(dplyr::filter(data,Location==each, Animal_Type==\"Bird\"))&gt;5){\n    cities = c(cities,each)\n  }\n}\n\nGampaha11 &lt;- filter(data, Location==\" Kadawatha,  Gampaha\", Animal_Type==\"Dog\" )[1:5,]\nGampaha21 &lt;- filter(data, Location==\" Kadawatha,  Gampaha\", Animal_Type==\"Cat\" )[1:5,]\nGampaha31 &lt;- filter(data, Location==\" Kadawatha,  Gampaha\", Animal_Type==\"Fish\" )[1:5,]\nGampaha41 &lt;- filter(data, Location==\" Kadawatha,  Gampaha\", Animal_Type==\"Bird\" )[1:5,]\n\nGampaha12 &lt;- filter(data, Location ==\" Kiribathgoda,  Gampaha\", Animal_Type==\"Dog\" )[1:5,]\nGampaha22 &lt;- filter(data, Location==\" Kiribathgoda,  Gampaha\", Animal_Type==\"Cat\")[1:5,]\nGampaha32 &lt;- filter(data, Location==\" Kiribathgoda,  Gampaha\", Animal_Type==\"Fish\")[1:5,]\nGampaha42 &lt;- filter(data, Location==\" Kiribathgoda,  Gampaha\", Animal_Type==\"Bird\" )[1:5,]\n\nKandy11 &lt;- filter(data, Location==\" Kandy City,  Kandy\", Animal_Type==\"Dog\" )[1:5,]\nKandy21 &lt;- filter(data, Location==\" Kandy City,  Kandy\", Animal_Type==\"Cat\" )[1:5,]\nKandy31 &lt;- filter(data, Location==\" Kandy City,  Kandy\", Animal_Type==\"Fish\" )[1:5,]\nKandy41 &lt;- filter(data, Location==\" Kandy City,  Kandy\", Animal_Type==\"Bird\" )[1:5,]\n\nKandy12 &lt;- filter(data, Location==\" Katugastota,  Kandy\", Animal_Type==\"Dog\" )[1:5,]\nKandy22 &lt;- filter(data, Location==\" Katugastota,  Kandy\", Animal_Type==\"Cat\" )[1:5,]\nKandy32 &lt;- filter(data, Location==\" Katugastota,  Kandy\", Animal_Type==\"Fish\" )[1:5,]\nKandy42 &lt;- filter(data, Location==\" Katugastota,  Kandy\", Animal_Type==\"Bird\" )[1:5,]\n\nColombo11 &lt;- filter(data, Location==\" Kotte,  Colombo\", Animal_Type==\"Dog\" )[1:5,]\nColombo21 &lt;- filter(data, Location==\" Kotte,  Colombo\", Animal_Type==\"Cat\" )[1:5,]\nColombo31 &lt;- filter(data, Location==\" Kotte,  Colombo\", Animal_Type==\"Fish\" )[1:5,]\nColombo41 &lt;- filter(data, Location==\" Kotte,  Colombo\", Animal_Type==\"Bird\" )[1:5,]\n\nColombo12 &lt;- filter(data, Location==\" Kolonnawa,  Colombo\", Animal_Type==\"Dog\" )[1:5,]\nColombo22 &lt;- filter(data, Location==\" Kolonnawa,  Colombo\", Animal_Type==\"Cat\"  )[1:5,]\nColombo32 &lt;- filter(data, Location==\" Kolonnawa,  Colombo\", Animal_Type==\"Fish\"  )[1:5,]\nColombo42 &lt;- filter(data, Location==\" Kolonnawa,  Colombo\", Animal_Type==\"Bird\"  )[1:5,]\n\ncombination &lt;-rbind(Gampaha11,Gampaha12,Gampaha21,Gampaha22,Gampaha31,Gampaha32,Gampaha41,Gampaha42,Kandy11,Kandy12,Kandy21,Kandy22,Kandy31,Kandy32,Kandy41,Kandy42,Colombo11,Colombo12,Colombo21,Colombo22,Colombo31,Colombo32,Colombo41,Colombo42) %&gt;%\n  mutate(Location = case_when(\n    Location == \" Kotte,  Colombo\" ~ \"Colombo\",\n    Location == \" Kolonnawa,  Colombo\" ~ \"Colombo\",\n    Location == \" Katugastota,  Kandy\" ~ \"Kandy\",\n    Location == \" Kandy City,  Kandy\" ~ \"Kandy\",\n    Location == \" Kiribathgoda,  Gampaha\" ~ \"Gampaha\",\n    Location == \" Kadawatha,  Gampaha\" ~ \"Gampaha\"\n  ))\n\n\ninferior_fit &lt;- aov(Price_in_Rupees~Animal_Type + Location, data=combination)\nanova(inferior_fit)\n\nAnalysis of Variance Table\n\nResponse: Price_in_Rupees\n             Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    \nAnimal_Type   3 4.7523e+10 1.5841e+10  7.4750 0.0001368 ***\nLocation      2 6.2369e+09 3.1184e+09  1.4715 0.2341891    \nResiduals   107 2.2676e+11 2.1192e+09                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(inferior_fit,conf.level=.975,which=\"Animal_Type\")\n\n  Tukey multiple comparisons of means\n    97.5% family-wise confidence level\n\nFit: aov(formula = Price_in_Rupees ~ Animal_Type + Location, data = combination)\n\n$Animal_Type\n               diff        lwr        upr     p adj\nCat-Bird   16044.20 -20596.886  52685.291 0.5918521\nDog-Bird   32166.67  -1969.075  66302.409 0.0390312\nFish-Bird -22037.10 -56172.842  12098.642 0.2542078\nDog-Cat    16122.46 -20518.625  52763.552 0.5879680\nFish-Cat  -38081.30 -74722.391  -1440.214 0.0182015\nFish-Dog  -54203.77 -88339.509 -20068.025 0.0000798\n\nfit&lt;- aov(sqrt(Price_in_Rupees)~Animal_Type + Location, data=combination)\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: sqrt(Price_in_Rupees)\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAnimal_Type   3 605119  201706 29.8031 4.379e-14 ***\nLocation      2  13783    6891  1.0182    0.3647    \nResiduals   107 724173    6768                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(fit,conf.level=.975,which=\"Animal_Type\")\n\n  Tukey multiple comparisons of means\n    97.5% family-wise confidence level\n\nFit: aov(formula = sqrt(Price_in_Rupees) ~ Animal_Type + Location, data = combination)\n\n$Animal_Type\n                diff        lwr        upr     p adj\nCat-Bird    97.89168   32.41165  163.37170 0.0002247\nDog-Bird   128.89603   67.89322  189.89884 0.0000001\nFish-Bird  -50.00844 -111.01125   10.99436 0.0924279\nDog-Cat     31.00435  -34.47568   96.48438 0.5272384\nFish-Cat  -147.90012 -213.38015  -82.42009 0.0000000\nFish-Dog  -178.90447 -239.90728 -117.90166 0.0000000\n\nCity &lt;- combination$Location\nAnimal_Type &lt;- combination$Animal_Type\nCost_in_Rupees &lt;- combination$Price_in_Rupees"
  },
  {
    "objectID": "Pets_SriLanka.html#data-analysis",
    "href": "Pets_SriLanka.html#data-analysis",
    "title": "Pet Ownership in Sri Lanka",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nAnalysis of Variance Table\n\nResponse: Price_in_Rupees\n             Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    \nAnimal_Type   3 4.7523e+10 1.5841e+10  7.4750 0.0001368 ***\nLocation      2 6.2369e+09 3.1184e+09  1.4715 0.2341891    \nResiduals   107 2.2676e+11 2.1192e+09                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    97.5% family-wise confidence level\n\nFit: aov(formula = Price_in_Rupees ~ Animal_Type + Location, data = combination)\n\n$Animal_Type\n               diff        lwr        upr     p adj\nCat-Bird   16044.20 -20596.886  52685.291 0.5918521\nDog-Bird   32166.67  -1969.075  66302.409 0.0390312\nFish-Bird -22037.10 -56172.842  12098.642 0.2542078\nDog-Cat    16122.46 -20518.625  52763.552 0.5879680\nFish-Cat  -38081.30 -74722.391  -1440.214 0.0182015\nFish-Dog  -54203.77 -88339.509 -20068.025 0.0000798\n\n\nAnalysis of Variance Table\n\nResponse: sqrt(Price_in_Rupees)\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAnimal_Type   3 605119  201706 29.8031 4.379e-14 ***\nLocation      2  13783    6891  1.0182    0.3647    \nResiduals   107 724173    6768                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    97.5% family-wise confidence level\n\nFit: aov(formula = sqrt(Price_in_Rupees) ~ Animal_Type + Location, data = combination)\n\n$Animal_Type\n                diff        lwr        upr     p adj\nCat-Bird    97.89168   32.41165  163.37170 0.0002247\nDog-Bird   128.89603   67.89322  189.89884 0.0000001\nFish-Bird  -50.00844 -111.01125   10.99436 0.0924279\nDog-Cat     31.00435  -34.47568   96.48438 0.5272384\nFish-Cat  -147.90012 -213.38015  -82.42009 0.0000000\nFish-Dog  -178.90447 -239.90728 -117.90166 0.0000000\n\n\nIn order to perform the F test, homoscedasticity across all sample groups under a specific model is an assumption that needs to be met. In terms of this analysis, that means the variance in pet prices within each subgroup needs to be similar.\n\ninferior_fit &lt;- aov(Price_in_Rupees~Animal_Type + Location, data=combination)\nplot(inferior_fit,which = 1)\n\n\n\n\n\n\n\n\nEach group of points in this figure represents a specific subgroup of pet prices corresponding to a specific type of pet in a specific city. When the spread of points within each subgroup is compared to the spread of them in other subgroups, it is evident that different subgroups have different spreads, meaning they have different variances in pet price.\n\nplot(fit,which = 1)\n\n\n\n\n\n\n\n\nAs this figure shows, the variances within each group resemble each other significantly more when prices are interpreted in terms of their square root. Since homoscedasticity is necessary for the F-test, the rest of this analysis will interpret pet prices in terms of their square roots instead of the prices themselves. By making sure that homoscedasticity is met, the potential effects of pet type and location on pet prices can then be seen.\nThe purpose of the ANOVA analysis was to see if either pet type or location individually had an effect on the prices of pets. For this reason, two F-tests were conducted for each possible variable. Each variable was given an alpha value of .025 in order to maintain a family-wise confidence level of 95%. The first F-test consists of mean square pet type divided by mean square error, which is the variance of the mean square-root prices in terms of pet type only for all observations divided by the variance of all residuals given the entire model is used to find the fitted value. This is a measure of the amount of variance in the full model explained by pet type being a factor divided by the variance of the residuals given the full model. If pet type is not a significant factor, the numerator of this value should be approximately equal to one variance, meaning the resulting F-score should be approximately one. The null hypothesis for this test is that pet type is not a significant factor, meaning there would be close to no difference in the mean square root prices per pet type. The p-value for the F-test corresponding to pet-type, however, was lower than .025, meaning this null hypothesis is most likely not true and pet type is a significant factor in this model. A second F-test was also performed to test whether a sale occurring in Gampaha, Kandy, or Colombo influenced square root pet prices; since the test resulted in a p-value greater than .025, the null hypothesis that residing in one of these three cities does not have a significant effect on square root pet prices cannot be dismissed.\n\nplot(TukeyHSD(fit,conf.level=.975,which=\"Animal_Type\"))\n\n\n\n\n\n\n\n\nFrom top to bottom, this figure presents confidence intervals for the following pairwise comparisons: cat-bird, dog-bird, fish-bird, dog-cat, fish-cat, and fish-dog. The family wise confidence level is 97.5% because comparisons concerning pet type all have an alpha value of .025. The confidence intervals for the cat-bird, dog-bird, fish-cat, and fish-dog comparisons do not cover zero, meaning this analysis suggests 97.5% confidence of these pet groups possessing different prices from one another throughout Colombo, Kandy, and Gampaha. Since the dog-cat and fish-bird comparisons cover zero, there does not appear to be a significant difference among both pairs of pet price averages throughout Colombo, Kandy, and Gampaha. Both of these sets of comparisons suggest there is an overall difference in price between mammalian pets and non-mammalian pets among the residents of Colombo, Kandy, and Gampaha, which corresponds to people in all three cities assigning differents levels of value to mammalian and non-mammalian pets. It is also possible that dogs and cats, as the most popular pets worldwide, have significantly higher prices in Colombo, Kandy, and Gampaha than all other pets, including other mammalian ones. In order to determine which hypothesis is accurate, however, more analysis would need to be conducted. Based on this analysis, an appropriate model for predicting the prices of pets in Gampaha, Kandy, and Colombo should consist of the average pet price per species of pet."
  },
  {
    "objectID": "Pets_SriLanka.html#conclusions",
    "href": "Pets_SriLanka.html#conclusions",
    "title": "Pet Ownership in Sri Lanka",
    "section": "Conclusions",
    "text": "Conclusions\nThe species of a pet appears to have a significant influence on its pricing in the cities of Gampaha, Kandy, and Colombo. Possible improvements to this analysis involve extending it to more cities in Sri Lanka through the inclusion of other cities or through redoing this analysis under a mixed-effects model. This would be in order to definitively exclude location as an influence on the prices of pets in Sri Lanka. Differences in pricing also seem to be most pronounced between the mammalian and non-mammalian species of the sample analyzed. In order to determine whether this difference is based on the status of a pet as a mammalian or non-mammalian animal, one-way ANOVA could be conducted comparing these two groups of pets."
  },
  {
    "objectID": "Pets_SriLanka.html#references",
    "href": "Pets_SriLanka.html#references",
    "title": "Pet Ownership in Sri Lanka",
    "section": "References",
    "text": "References\nJayawardena, Lasal. “Sri Lanka Animal Prices Dataset.” Kaggle, 31 Oct. 2021, www.kaggle.com/datasets/lasaljaywardena/sri-lanka-animal-prices-dataset.\nKonstantinova, Anastasia, et al. “Perceived Benefits and Costs of Owning a Pet in a Megapolis: An Ecosystem Services Perspective.” MDPI, 24 Sept. 2021, www.mdpi.com/2071-1050/13/19/10596.\nRathish, Devarajan, et al. “Household Preferences for Pet Keeping: Findings from a Rural District of Sri Lanka.” PLOS ONE, 22 Nov. 2022, journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0277108."
  },
  {
    "objectID": "SIBDS.html",
    "href": "SIBDS.html",
    "title": "Summer Institute in Biostatistics and Data Science Research",
    "section": "",
    "text": "During the summer of 2024, I participated in the Summer Institute in Biostatistics and Data Science Research (SIBDS) at Columbia University’s Mailman School of Public Health. I performed research alongside Hunter Farnham, another participant in this program, and was guided by Dr. Alan Cohen and graduate student Christine Kuryla. The poster below describes the results of the research we performed.\nDownload PDF file."
  },
  {
    "objectID": "tda_research.html",
    "href": "tda_research.html",
    "title": "Kenneth Cooke Fellowship Research",
    "section": "",
    "text": "During the summer of 2023, I assisted Professor Chandler in performing research on Topological Data Analysis under the Kenneth Cooke Fellowship. The results of said research are described in the document below.\nDownload PDF file."
  }
]