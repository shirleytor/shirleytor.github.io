---
title: "Simulation Study"
subtitle: "Using Normal Prediction Intervals on Symmetric Beta-distributed data"
author: "Shirley Toribio"
format: html
editor: visual
---

# Project 3

## Introduction

Symmetric beta distributions and normal distributions are very similar to each other in shape. Due to this, in this project I examine how a 95% prediction interval that assumes data originates from a normal distribution would fare in terms of prediction accuracy and actual coverage probability of beta-distributed data. I define a confidence interval with "good" prediction accuracy as one that covers the actual mean of the data's parent distribution, and a confidence interval with good coverage probability as one that covers approximately .95 of the beta distribution. All data is generated from beta distributions with mean .5.

## Simulations

First, I tidy up.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```

Then, I define a few functions that will help me in my task.

```{r, warning=FALSE, message=FALSE}
CI <- function(data, coverage_prob){ 
  #Generates a normal prediction interval with an intended coverage probability of coverage_prob based on a vector of numeric data
  
  lower_zscore <- qnorm((1-coverage_prob)/2)
  upper_zscore <- qnorm(((1-coverage_prob)/2) + coverage_prob)
  avg <- mean(data)
  stan_d <- sd(data)
  lower_bound <- avg + lower_zscore*stan_d
  upper_bound <- avg + upper_zscore*stan_d
  return(data.frame(PI_percentage = coverage_prob, lower = lower_bound, upper = upper_bound))
}

one_beta_simulation <- function(n, alpha, beta, ci_prop){
  #Assesses prediction accuracy and actual coverage probability of a normal prediction interval when used on a vector of numeric data of size n. The numeric data is generated from a beta distribution with parameters alpha and beta.
  
  cover_df <- CI(rbeta(n, alpha, beta), ci_prop)
  cover_prop <- pbeta(cover_df[1, "upper"], alpha, beta) - pbeta(cover_df[1, "lower"], alpha, beta)
  mean_in_interval <- .5 >= cover_df[1, "lower"] & .5 <= cover_df[1,"upper"]
  param_df <- data.frame(cover = cover_prop, alpha = rep(alpha, nrow(cover_df)), beta = rep(beta, nrow(cover_df)), mean_in_interval = mean_in_interval)
  df <- cbind(cover_df, param_df)
  return(df)
}

beta_sims_n <- function(n){
  #Iterates over a vector of possible alpha = beta values and applies one_beta_simulation to each possible value of alpha/beta. All simulations use data of sample size n.
  df1 <- map(parameters,\(param) one_beta_simulation(n, param, param, ci) ) %>%
  list_rbind()
  df2 <- data.frame(n = rep(n, nrow(df1)))
  df <- cbind(df2, df1)
  return(df)
}
```

Time to simulate over different parameter values and sample sizes!

```{r, warning=FALSE, message=FALSE}
parameters <- seq(5, 200, by = 2)
n <- 2:500
ci <- .95

beta_df <- map(n, \(n) beta_sims_n(n)) %>%
  list_rbind()
```

This is a glimpse at the results of the simulations.

```{r, warning=FALSE, message=FALSE}
rows <- sample(1:nrow(beta_df), 10)
map(rows, \(i) beta_df[i,]) %>%
  list_rbind()
```

This is a random sample of rows from the actual dataset, which has 48902 rows and 8 columns. Each row corresponds to a simulated random sample of size n from a beta distribution with parameters alpha and beta. For each random sample, a normal prediction interval was generated with bounds "lower" and "upper". "PI_percentage" refers to the intended coverage probability and "cover" refers to the actual coverage probability of said prediction interval over the beta distribution the data was generated from. "mean_in_interval" is a binary variable that states if the prediction interval covered the mean of the distribution.

## Insights

```{r, warning=FALSE, message=FALSE}
n_means_df <- beta_df %>%
  mutate(diff = cover - PI_percentage) %>%
  group_by(n) %>%
  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n()) %>%
  filter(n %in% 1:100)

ggplot(n_means_df, aes(x = n, y = mean)) + 
  geom_point() + 
  geom_hline(yintercept = 0, col = "black")  +
  labs(
    x = "sample size",
    y = "difference between actual and intended coverage",
    title = "Figure 1",
    subtitle = "difference between actual and intended coverage probability based on sample size",
  )
```

Figure 1 graphs the mean difference between the actual coverage probability and intended coverage probability (calculated as: actual coverage probability - intended coverage probability) per each sample size from 2 to 100. A negative mean difference indicates the actual coverage probability tended to be less than the intended coverage probability, which is undesirable. For sample sizes from 2 to around 13, the mean differences tend to be much lower than 0, meaning the prediction interval is likely to cover less than intended. As the sample size increases, this mean difference seems to converge in probability to 0, meaning the actual coverage probability is more likely to match the intended coverage probability.

For small sample sizes it seems likely a normal prediction interval will cover less than intended, with it seeming to cover less the smaller the sample size is.

```{r, warning=FALSE, message=FALSE}
n_means_df <- beta_df %>%
  mutate(diff = cover - PI_percentage) %>%
  group_by(n) %>%
  summarize(mean = mean(diff), mu_in_interval = sum(mean_in_interval)/n()) %>%
  filter(n %in% 1:30)

ggplot(n_means_df, aes(x = n, y = mu_in_interval)) + 
  geom_point()  +
  labs(
    x = "sample size",
    y = "proportion of mu-inclusive prediction intervals",
    title = "Figure 2",
    subtitle = "proportion of mu-inclusive prediction intervals based on sample size"
  )
  
```

Figure 2 plots the proportion of prediction intervals that cover mu = .5 per each sample size from 2 to 30. These points converge to 1 at a sample size of around 10, meaning for sample sizes of 10 or greater it is probable that all random samples with those samples sizes will produce normal-prediction intervals that cover the mean of the beta distribution said data originated from. Based on this plot, the normal-prediction interval fares well with accepting the null hypothesis of mu = .5 given the null distribution is symmetric and beta.

```{r, warning=FALSE, message=FALSE}
ns_of_interest <- c(5, 10, 30, 50, 100, 500)
beta_df_2 <- filter(beta_df,n %in% ns_of_interest) %>%
  mutate(sample_size = as.factor(n))
ggplot(beta_df_2, aes(x = alpha, y = cover, color = sample_size)) +
  geom_point() +
  geom_smooth(aes(line = n ), se = FALSE) +
  labs(
    x = "value of alpha and beta",
    y = "actual coverage probability",
    title = "Figure 3",
    subtitle = "actual coverage probability based on parameter values"
  )


```

Figure 3 plots the actual coverage probability per each value of alpha and beta. Points and lines are colored based on sample size, which are described in the legend. Alpha and beta have the same values, by the way. All the lines of best fit seem close to being horizontal, meaning that there most likely is not a relationship between the values of alpha/beta and the actual coverage probability of the data. Based on the y-intercepts of each line and the vertical spread of points given sample size, normal prediction intervals based on small amounts of data seem more likely to deviate from the intended coverage probability of the data's parent symmetric beta distribution and to deviate to being lower than .95.

## Conclusion

By generating data from symmetric beta distributions with mu = .5 and making normal prediction intervals based on this data, I was able to assess the predictive accuracy and actual coverage probability of normal prediction intervals when applied to symmetric-beta data. Even for sample sizes as small as 10, normal prediction intervals seem to have good predictive accuracy, although their coverage probability is quite poor for sample sizes below 30. When the null hypothesis is mu = .5, it appears they are quite adept at avoiding type I errors.
